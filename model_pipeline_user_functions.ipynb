{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10344121,"sourceType":"datasetVersion","datasetId":6405577}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T09:21:08.866935Z","iopub.execute_input":"2026-02-03T09:21:08.867145Z","iopub.status.idle":"2026-02-03T09:21:10.494562Z","shell.execute_reply.started":"2026-02-03T09:21:08.867122Z","shell.execute_reply":"2026-02-03T09:21:10.493521Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart/Walmart.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/walmart/Walmart.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T09:21:10.496703Z","iopub.execute_input":"2026-02-03T09:21:10.497230Z","iopub.status.idle":"2026-02-03T09:21:10.577190Z","shell.execute_reply.started":"2026-02-03T09:21:10.497199Z","shell.execute_reply":"2026-02-03T09:21:10.576290Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def modeling():\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import time\n    from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n    from sklearn.preprocessing import StandardScaler,OneHotEncoder, OrdinalEncoder\n    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n\n    \n    \n    # ML Modelleri\n    from sklearn.linear_model import LinearRegression\n    from sklearn.tree import DecisionTreeRegressor\n    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n    from xgboost import XGBRegressor\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.svm import SVR\n    from sklearn.neural_network import MLPRegressor\n\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n\n    size_order = [['Budget','Mid-Range','Upper Mid-Range','Luxury','Ultra Luxury']]\n    \n    # Create column transformer\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('onehot', OneHotEncoder(), nominal_features),\n            ('ordinal', OrdinalEncoder(categories=size_order), ordinal_features),\n        ],\n        remainder='passthrough'\n    )\n\n    X_train = preprocessor.fit_transform(X_train)\n    X_test = preprocessor.transform(X_test)\n\n    \n    Sc = StandardScaler()\n    X_train = Sc.fit_transform(X_train)\n    X_test = Sc.transform(X_test)\n\n    models = {\n    \"Linear Regression\": LinearRegression(),\n    \"Decision Tree\": DecisionTreeRegressor(max_depth=10, min_samples_split=4, random_state=42),\n    \"Random Forest\": RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42),\n    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42),\n    \"XGBoost\": XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42),\n    \"KNN\": KNeighborsRegressor(n_neighbors=10, weights=\"distance\", metric=\"minkowski\"),\n    \"SVR\": SVR(kernel='rbf', C=100, epsilon=0.1, gamma='scale'),\n    \"Neural Network (MLP)\": MLPRegressor(hidden_layer_sizes=(128, 64, 32), activation='relu', solver='adam', max_iter=1000, random_state=42)\n    }\n\n    results = []\n\n    for name, model in models.items():\n        print(f\"Training {name}...\")\n    \n        # Training\n        start_time = time.time()\n        model.fit(X_train, y_train)\n        train_time = time.time() - start_time\n        \n        # Prediction\n        start_time = time.time()\n        y_pred = model.predict(X_test)\n        predict_time = time.time() - start_time\n        \n        # Performance Metrics\n        mae = mean_absolute_error(y_test, y_pred)\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n    \n        # Store results\n        results.append([name, mae, mse, r2, train_time, predict_time])\n    \n    results_df = pd.DataFrame(results, columns=[\"Model\", \"MAE\", \"MSE\", \"R² Score\", \"Training Time (sec)\", \"Prediction Time (sec)\"])\n    from IPython.display import display\n    display(results_df)\n\n    plt.figure(figsize=(12,6))\n    sns.barplot(data=results_df, x=\"Model\", y=\"R² Score\", palette=\"viridis\")\n    plt.xticks(rotation=45)\n    plt.title(\"Optimized Model Comparison - R² Score\")\n    plt.show()\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def early_stage:\n    from IPython.display import display\n    display(df)\n    display(df.info())\n    display(df.dtypes)\n    display(df.isnull().sum())\n    display(df.isduplicate())\n\ndef EDA(num_cols,cat_cols,df):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    if cat_cols:\n        num_cols = 2  # Number of subplots per row\n        total = len(columns)\n        num_rows = math.ceil(total / num_cols)\n    \n        plt.figure(figsize=(num_cols * 5, num_rows * 4))\n    \n        for idx, col in enumerate(columns):\n            plt.subplot(num_rows, num_cols, idx + 1)\n            sns.countplot(data=df, x=col, order=df[col].value_counts().index)\n            plt.title(col)\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n    \n        plt.show()    \n    \n        pairs = list(itertools.combinations(columns, 2))\n        total = len(pairs)\n        num_cols = 2  # 2 plots per row\n        num_rows = math.ceil(total / num_cols)\n    \n        plt.figure(figsize=(num_cols * 6, num_rows * 5))\n    \n        for idx, (col1, col2) in enumerate(pairs):\n            plt.subplot(num_rows, num_cols, idx + 1)\n            sns.countplot(data=df, x=col1, hue=col2)\n            plt.title(f\"{col1} vs {col2}\")\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n    \n        plt.show()\n\n    if num_cols:\n        for col in columns:\n            fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n            \n            # Title for the whole row\n            fig.suptitle(col, fontsize=14, fontweight='bold', y=1.05)\n    \n            # Histogram\n            sns.histplot(df[col], kde=True, bins=30, ax=axs[0])\n            axs[0].set_title('Histogram')\n            axs[0].set_xlabel(col)\n            axs[0].set_ylabel('Frequency')\n    \n            # Boxplot\n            sns.boxplot(y=df[col], ax=axs[1])\n            axs[1].set_title('Boxplot')\n            axs[1].set_ylabel(col)\n    \n            plt.tight_layout()\n            plt.show()\n\n        \n        pairs = list(itertools.combinations(columns, 2))\n        total = len(pairs)\n        num_cols = 2\n        num_rows = math.ceil(total / num_cols)\n    \n        plt.figure(figsize=(num_cols * 6, num_rows * 5))\n    \n        for idx, (col1, col2) in enumerate(pairs):\n            plt.subplot(num_rows, num_cols, idx + 1)\n            sns.scatterplot(data=df, x=col1, y=col2)\n            plt.title(f\"{col1} vs {col2}\")\n            plt.tight_layout()\n    \n        plt.show()\n\n    pairs = list(itertools.product(cat_cols, num_cols))\n\n    if cat_cols and num_cols:\n        for cat_col, num_col in pairs:\n            plt.figure(figsize=(12, 5))\n            \n            # Title for this pair\n            plt.suptitle(f'{num_col} vs {cat_col}', fontsize=14, fontweight='bold')\n    \n            # --- Left: Distribution plot (histogram per category) ---\n            plt.subplot(1, 2, 1)\n            sns.kdeplot(x=df[num_col], hue=df[cat_col],common_norm=False)\n            plt.title(f'Distribution of {num_col} by {cat_col}')\n    \n            # --- Right: Barplot (mean of num_col per category) ---\n            plt.subplot(1, 2, 2)\n            sns.barplot(data=df, x=cat_col, y=num_col, estimator='mean', ci='sd')\n            plt.title(f'Mean {num_col} by {cat_col}')\n            plt.xticks(rotation=45)\n    \n            plt.tight_layout()\n            plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grids = {\n\n    \"LinearRegression\": {\n        \"fit_intercept\": [True, False],\n        \"positive\": [True, False]\n    },\n\n    \"Ridge\": {\n        \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100],\n        \"solver\": [\"auto\", \"svd\", \"cholesky\", \"lsqr\"]\n    },\n\n    \"Lasso\": {\n        \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n        \"max_iter\": [1000, 5000, 10000]\n    },\n\n    \"ElasticNet\": {\n        \"alpha\": [0.001, 0.01, 0.1, 1, 10],\n        \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n        \"max_iter\": [1000, 5000]\n    },\n\n    \"DecisionTreeRegressor\": {\n        \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\"],\n        \"max_depth\": [None, 5, 10, 20, 30],\n        \"min_samples_split\": [2, 5, 10],\n        \"min_samples_leaf\": [1, 2, 5],\n        \"max_features\": [None, \"sqrt\", \"log2\"]\n    },\n\n    \"RandomForestRegressor\": {\n        \"n_estimators\": [100, 300, 500],\n        \"max_depth\": [None, 10, 20, 30],\n        \"min_samples_split\": [2, 5, 10],\n        \"min_samples_leaf\": [1, 2, 4],\n        \"max_features\": [\"sqrt\", \"log2\"],\n        \"bootstrap\": [True, False]\n    },\n\n    \"GradientBoostingRegressor\": {\n        \"n_estimators\": [100, 300, 500],\n        \"learning_rate\": [0.01, 0.05, 0.1],\n        \"max_depth\": [3, 5, 7],\n        \"subsample\": [0.6, 0.8, 1.0],\n        \"min_samples_split\": [2, 5, 10]\n    },\n\n    \"XGBRegressor\": {\n        \"n_estimators\": [200, 500, 800],\n        \"learning_rate\": [0.01, 0.05, 0.1],\n        \"max_depth\": [3, 5, 7, 10],\n        \"subsample\": [0.6, 0.8, 1.0],\n        \"colsample_bytree\": [0.6, 0.8, 1.0],\n        \"reg_alpha\": [0, 0.1, 1],     # L1 regularization\n        \"reg_lambda\": [1, 5, 10]     # L2 regularization\n    },\n\n    \"KNeighborsRegressor\": {\n        \"n_neighbors\": [3, 5, 7, 9, 15],\n        \"weights\": [\"uniform\", \"distance\"],\n        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]\n    },\n\n    \"SVR\": {\n        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n        \"C\": [0.1, 1, 10, 100],\n        \"epsilon\": [0.01, 0.1, 0.2],\n        \"gamma\": [\"scale\", \"auto\"]\n    },\n\n    \"MLPRegressor\": {\n        \"hidden_layer_sizes\": [(50,), (100,), (50,50), (100,50)],\n        \"activation\": [\"relu\", \"tanh\"],\n        \"solver\": [\"adam\", \"lbfgs\"],\n        \"alpha\": [0.0001, 0.001, 0.01],   # L2 regularization\n        \"learning_rate\": [\"constant\", \"adaptive\"],\n        \"max_iter\": [500, 1000]\n    }\n}\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n\nMODELS = {\n    \"LinearRegression\": LinearRegression(),\n    \"Ridge\": Ridge(),\n    \"Lasso\": Lasso(),\n    \"ElasticNet\": ElasticNet(),\n    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=42),\n    \"RandomForestRegressor\": RandomForestRegressor(random_state=42),\n    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=42),\n    \"XGBRegressor\": XGBRegressor(\n        objective=\"reg:squarederror\",\n        eval_metric=\"rmse\",\n        random_state=42\n    ),\n    \"KNeighborsRegressor\": KNeighborsRegressor(),\n    \"SVR\": SVR(),\n    \"MLPRegressor\": MLPRegressor(random_state=42)\n}\n\nfrom sklearn.model_selection import GridSearchCV\n\ndef run_grid_search(\n    model_name,\n    X_train,\n    y_train,\n    cv=5,\n    scoring=\"neg_mean_squared_error\",\n    n_jobs=-1,\n    verbose=1\n):\n    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n    from sklearn.tree import DecisionTreeRegressor\n    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.svm import SVR\n    from sklearn.neural_network import MLPRegressor\n    from xgboost import XGBRegressor\n    \n    \n    MODELS = {\n        \"LinearRegression\": LinearRegression(),\n        \"Ridge\": Ridge(),\n        \"Lasso\": Lasso(),\n        \"ElasticNet\": ElasticNet(),\n        \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=42),\n        \"RandomForestRegressor\": RandomForestRegressor(random_state=42),\n        \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=42),\n        \"XGBRegressor\": XGBRegressor(\n            objective=\"reg:squarederror\",\n            eval_metric=\"rmse\",\n            random_state=42\n        ),\n        \"KNeighborsRegressor\": KNeighborsRegressor(),\n        \"SVR\": SVR(),\n        \"MLPRegressor\": MLPRegressor(random_state=42)\n    }\n    param_grids = {\n\n    \"LinearRegression\": {\n        \"fit_intercept\": [True, False],\n        \"positive\": [True, False]\n    },\n\n    \"Ridge\": {\n        \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100],\n        \"solver\": [\"auto\", \"svd\", \"cholesky\", \"lsqr\"]\n    },\n\n    \"Lasso\": {\n        \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n        \"max_iter\": [1000, 5000, 10000]\n    },\n\n    \"ElasticNet\": {\n        \"alpha\": [0.001, 0.01, 0.1, 1, 10],\n        \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n        \"max_iter\": [1000, 5000]\n    },\n\n    \"DecisionTreeRegressor\": {\n        \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\"],\n        \"max_depth\": [None, 5, 10, 20, 30],\n        \"min_samples_split\": [2, 5, 10],\n        \"min_samples_leaf\": [1, 2, 5],\n        \"max_features\": [None, \"sqrt\", \"log2\"]\n    },\n\n    \"RandomForestRegressor\": {\n        \"n_estimators\": [100, 300, 500],\n        \"max_depth\": [None, 10, 20, 30],\n        \"min_samples_split\": [2, 5, 10],\n        \"min_samples_leaf\": [1, 2, 4],\n        \"max_features\": [\"sqrt\", \"log2\"],\n        \"bootstrap\": [True, False]\n    },\n\n    \"GradientBoostingRegressor\": {\n        \"n_estimators\": [100, 300, 500],\n        \"learning_rate\": [0.01, 0.05, 0.1],\n        \"max_depth\": [3, 5, 7],\n        \"subsample\": [0.6, 0.8, 1.0],\n        \"min_samples_split\": [2, 5, 10]\n    },\n\n    \"XGBRegressor\": {\n        \"n_estimators\": [200, 500, 800],\n        \"learning_rate\": [0.01, 0.05, 0.1],\n        \"max_depth\": [3, 5, 7, 10],\n        \"subsample\": [0.6, 0.8, 1.0],\n        \"colsample_bytree\": [0.6, 0.8, 1.0],\n        \"reg_alpha\": [0, 0.1, 1],     # L1 regularization\n        \"reg_lambda\": [1, 5, 10]     # L2 regularization\n    },\n\n    \"KNeighborsRegressor\": {\n        \"n_neighbors\": [3, 5, 7, 9, 15],\n        \"weights\": [\"uniform\", \"distance\"],\n        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]\n    },\n\n    \"SVR\": {\n        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n        \"C\": [0.1, 1, 10, 100],\n        \"epsilon\": [0.01, 0.1, 0.2],\n        \"gamma\": [\"scale\", \"auto\"]\n    },\n\n    \"MLPRegressor\": {\n        \"hidden_layer_sizes\": [(50,), (100,), (50,50), (100,50)],\n        \"activation\": [\"relu\", \"tanh\"],\n        \"solver\": [\"adam\", \"lbfgs\"],\n        \"alpha\": [0.0001, 0.001, 0.01],   # L2 regularization\n        \"learning_rate\": [\"constant\", \"adaptive\"],\n        \"max_iter\": [500, 1000]\n    }\n}\n    if model_name not in MODELS:\n        raise ValueError(f\"Model '{model_name}' not supported\")\n\n    model = MODELS[model_name]\n    param_grid = param_grids[model_name]\n\n    grid = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=scoring,\n        n_jobs=n_jobs,\n        verbose=verbose\n    )\n\n    grid.fit(X_train, y_train)\n\n    return {\n        \"best_model\": grid.best_estimator_,\n        \"best_params\": grid.best_params_,\n        \"best_score\": grid.best_score_\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}